---
title: "Weight Lifting Exercise Analysis"
author: "Van Hai Ho"
date: "August 18, 2014"
output:
  html_document:
    css: markdown_vhh.css
    fig_height: 4
    fig_width: 6
    keep_md: yes
    number_sections: yes
---

# Summary

Regular exercise is recommended even for healthy adults. People often quantify *how much* a particular exercise activity they do, but they rarely quantify *how well* they do it. This report provides an analysis and attempts to predict the manner in which people did the exercise.

The classification models built for analysis in this report are based on the Weight Lifting Exercise data set available at <https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv>. The classifiers was tested with 10-fold cross-validation. Random Forest algorithm scores the best accuracy, with overall accuracy of around 99.2%.

# Data Processing 

## Setting Environment <a name="settingenv"></a>

We started the analysis with a clean environment. Working directory was set and the required packages were loaded into the environment to be used for analysis.

```{r settingEnvironment, echo = TRUE, results='hide', message=FALSE, cache = TRUE}
# Clean up the environment
rm(list = ls())
# Set working directory
setwd("K:/Passage/training/coursera/Data_Science/08_Practical_Machine_Learning/project/project_writeup")

library(caret); library(kernlab); library(randomForest); 
library(plyr); library(glmnet); library(rpart); library(ggplot2)
library(doParallel)
cl <- makePSOCKcluster(4)
registerDoParallel(cl)
```

## Getting and Cleaning Data <a name="gettingAndCleaningData"></a>

The analysis in this report acquires the data set from the following link:

* Training data: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv
* Test data: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

```{r downloadDataSet, echo = TRUE, cache = TRUE}
# Check if the data file has been downloaded to working directory.
# If not, download it from the course website
trainDataFile <- "pml-training.csv"
if (!file.exists(trainDataFile)) {
    fileUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
    download.file(fileUrl, destfile=trainDataFile)
}

testDataFile <- "pml-testing.csv"
if (!file.exists(testDataFile)) {
    fileUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
    download.file(fileUrl, destfile=testDataFile)
}
```

The training data set contains 19,622 observations with 160 variables on six (6) users. This data set contains empty fields and <code>#DIV/0!</code> which will be considered as missing values <code>NA</code> in this report.

```{r loadData, cache = TRUE}
# Load data
trainData <- read.csv(trainDataFile, header = TRUE, sep = ",", 
                      na.strings = c("NA", "", "#DIV/0!"));
testData <- read.csv(testDataFile, header = TRUE, sep = ",", 
                     na.strings = c("NA", "", "#DIV/0!"))
```

There are five (5) classes: A, B, C, D, and E to classify how well an activity was performed by the user. Class A corresponds to the specified execution of the exercise, while the other 4 classes correspond to common mistakes:

* Class A: exactly according to the specification,
* Class B: throwing the elbows to the front,
* Class C: lifting the dumbbell only halfway,
* Class D: lowering the dumbbell only halfway, and
* Class E: throwing the hips to the front.

The measurements in the data set collected from the following:

* 4 sensors: belt, arm, dumbbell, glove
* 3 calculated Euler angles and 3 raw features for each sensor: roll, pitch, yaw, accelerometer, gyroscope and magnetometer;
* 8 calculated features: mean, variance, standard deviation, max, min, amplitude, kurtosis and skewness.

There are many variables that are mostly filled with missing values. The variables with more than 95% of missing values are excluded from the data used to build the models. user_name, time and windows related fields are not needed for building the models, therefore they are excluded as well.

```{r cleanData, cache = TRUE}
# Exclude variables with more than 95% of missing ("NA"") values
# e.g. only include variables with less than 5% of missing values
trainingRows <- nrow(trainData)
cleanedTrainData <- trainData[, (colSums(is.na(trainData)) < trainingRows * 0.05)]

# Exclude variables that are not used for classification
unusedVars <- c("X", "user_name", "raw_timestamp_part_1", "raw_timestamp_part_2", 
                "cvtd_timestamp", "new_window", "num_window")
cleanedTrainData <- cleanedTrainData[, -which(names(cleanedTrainData) %in% unusedVars)]
cleanedVars <- names(cleanedTrainData)

# Select variables for test set
cleanedTestData <- testData[, which(names(testData) %in% cleanedVars)]
cleanedTestData$problem_id <- testData[, "problem_id"]
```

The training data set was splitted into 2 parts: training and validating. The training part was used to build the models. The validating part was used to validate the performance of the models.

```{r partitionData, cache=TRUE}
set.seed(2122)
inTrain <- createDataPartition(y = cleanedTrainData$classe, p = 0.60, list = FALSE)
training <- cleanedTrainData[inTrain, ]
validating <- cleanedTrainData[-inTrain, ]
```

# Classification Models

In this section, we considered three classification models to recognise and predict the manner of how users perform their exercise acticity. 

## Random Forest (rf)

The data used in this analysis are sensor data which are often noisy. Hence the Random Forest approach was chosen to tackle this characteristic of noisy sensor data. In Random Forest approach, training algorithm searches over a random subset with the same distribution when splitting a node to build the trees in the forest. It is then combined with randomised node optimisation to generate classification result. 

To build the random forest, we first tried to find the optimal numbers of variables to try splitting on at each node. These numbers were used to tune up the model. To improve the performance, we used 10-fold cross-validation with <code>repeatedcv</code> method.

```{r rfModel, results='hide', message=FALSE, cache=TRUE}
#  find the optimal numbers of variables to try splitting on at each node
bestmtry <- tuneRF(training[-53], training$classe, ntreeTry = 100, 
                   stepFactor = 1.5, improve = 0.01, trace = FALSE, 
                   plot = FALSE, dobest = FALSE)

# Use bestmtry as tuning parameters for random forest
tuneParams <- data.frame("mtry" = bestmtry[, 1])

# Use 10 folds cross-validation 
rfCtrl <- trainControl(method = "repeatedcv", number = 10)

# Run the algorithm to train the model
trainRF <- train(classe ~ ., data = training, method = "rf",
                 preProcess = c("center", "scale"),
                 trControl = rfCtrl, tuneGrid = tuneParams)
```

The performance of <code>rf</code> model is shown in the following graph. All four models trained for the random forest achieve more 90% accuracy. The best model scores 99.2% accuracy.

```{r rfModelPlot, echo=FALSE, cache=TRUE}
ggplot(trainRF) + ggtitle("Random Forest Performance")
```

With random forest best model, the expected out of sample error is quite low, less than 1%:
```{r rfFinalModel, echo=FALSE, cache=TRUE}
trainRF$finalModel
```

Next, we validated the <code>rf</code> training model on <code>validating</code> data:

```{r rfValidation, cache=TRUE}
valPred <- predict(trainRF, newdata = validating)
valCfm <- confusionMatrix(valPred, reference = validating$classe)
```

The result of prediction using the best model for <code>rf</code> algorithm on the validating data is shown below:

```{r rfValidationTable, echo=FALSE, cache = TRUE}
# Result
valCfm$table
```

The overal accuracy of the prediction using the best model of <code>rf</code> algorithm on the validating data set:

```{r rfValidationAccuracy, echo=FALSE, cache=TRUE}
valCfm$overall[1]
```

## Recursive Partitioning and Regression Trees (rpart2)

The training data set contains large number of variables. Some variables might influence the others. Hence <code>rpart2</code> model is chosen.

```{r rpartModel, cache=TRUE}
## rpart classification
trainCART <- train(classe ~ ., data = training, method = "rpart2",
                  preProcess = c("center", "scale"), trControl = rfCtrl)

# Validating model
valPred <- predict(trainCART, newdata = validating)
valCfm <- confusionMatrix(valPred, validating$classe)
```

The performance of <code>rpart2</code> training model:

```{r rpartModelPlot, echo=FALSE, cache = TRUE}
ggplot(trainCART) + ggtitle("rpart2 Performance")
```

The result of prediction using the best model of <code>rpart2</code> on the validating data is shown in the table below.

```{r rpartValidationTable, echo=FALSE, cache = TRUE}
# Result
valCfm$table
```

The overal accuracy of the prediction using the best model of <code>rpart2</code> on the validating data set:

```{r rpartValidationAccuracy, echo=FALSE, cache=TRUE}
valCfm$overall[1]
```

## Generalized Linear Model via Penalized Maximum Likelihood (glmnet)

Generalized Linear Model via penalized maximum likelihood can deal with all shapes of data, including very large sparse data matrices. Thus, it was chosen to see if it works well for noisy sensor data used in this report.

```{r glmnetModel, cache=TRUE}
## generalized linear model
trainGlmnet <- train(classe ~ ., data = training, method = "glmnet",
                   preProcess = c("center", "scale"), trControl = rfCtrl)

# Validating model
valPred <- predict(trainGlmnet, newdata = validating)
valCfm <- confusionMatrix(valPred, validating$classe)
```

The performance of <code>glmnet</code> training model:

```{r glmnetModelPlot, echo=FALSE, cache = TRUE}
ggplot(trainGlmnet) + ggtitle("glmnet Performance")
```

The result of prediction using the best model of <code>glmnet</code> on the validating data is shown in the table below:

```{r glmnetValidationTable, echo=FALSE, cache = TRUE}
# Result
valCfm$table
```

The overal accuracy of the prediction using the best model of <code>glmnet</code> on the validating data set:

```{r glmnetValidationAccuracy, echo=FALSE, cache=TRUE}
valCfm$overall[1]
```

Amongst three classification models validated in this report, Random Forest yields the best accuracy. We will use this model to generate prediction for the test data.

# Prediction

In this section, we used Random Forest model, the best training model built in previous section, to predict the outcome for 20 test cases provided in <https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv>.

```{r predictTest, cache=TRUE}
predict(trainRF, newdata = cleanedTestData)
```

The accuracy of the predictions for this test set is measured in the submission part of the project.

# Conclusion

Quality of the exercise activity can be automatically measured and predicted in order to give feedback to users to improve their activity. With the data set used in this report, random forest algorithm yields the best prediction accuracy.

